---
title: "Machine Learning 1"
author: "Leyna Nguyen (A15422197)"
date: "10/21/2021"
output: html_document
---

First up is clustering methods

# Kmeans clustering

The function in base R to do Kmeans clustering is called `kmeans()`.

First make up some data where we know what the answer should be: 

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

> Q. Can we use kmeans() to cluster this data setting k 2 and nstart to 20?

```{r}
km <- kmeans(x, centers = 2, nstart = 20)
km
```
> Q. How many points are in each cluster?

```{r}
km$size
```
> Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```
> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```
> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

# Hierarchical Clustering

A big limitation with k-means is that we have to tell it K (the number of clusters we want).

Analyze this same data with hclust()

Demonstrate the use of dist(), hclust(), plot(), and cutree() functions to do clustering. 
Generate dendrograms and return cluster assignment/membership vector...

```{r}
hc <- hclust(dist(x))
hc
```

There is a plot method for hclust result objects. Let's see it

```{r}
plot(hc)
```

To get our cluster membership vector we have to do a wee bit more work. We have to "cut" the tree where we think it makes sense. For this we use the `cutree()` function.

```{r}
cutree(hc, h=6)
```

You can also call `cutree()` setting k=the number of groups/clusters you want

```{r}
grps <- cutree(hc, k=2)
```

Make our results plot

```{r}
plot(x, col=grps)
```

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

Checking the data
```{r}
View(x)
```

# Note how the minus indexing works
```{r}
rownames(x) <- x[, 1]
x <- x[, -1]
head(x)
```

Check the dimensions
```{r}
dim(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
I prefer the 2nd approach because 

Now we have the data looking good, we want to explore it. We will use some conventional plots (barplots and pair plots).
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

Changing the argument beside=FALSE in the barplot() function will result in a barplot with stacked bars instead of juxtaposed bars. 

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

# PCA to the rescue!

The main function in base R for PCA is `prcomp()` 
This wants the transpose of our data

# Use the prcomp() PCA function 

```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

# Plot PC1 vs PC2
```{r}
plot(pca$x[, 1], pca$x[, 2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[, 1], pca$x[, 2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("yellow", "red", "blue", "dark green"))
```






